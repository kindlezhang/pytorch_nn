import torch 
from torch import nn
from torchsummary import summary
# from torchinfo import summary

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5, padding = 2)
        self.sig = nn.Sigmoid()
        self.s2 = nn.AvgPool2d(kernel_size = 2, stride = 2)
        self.conv3 = nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = 5)
        self.s4 = nn.AvgPool2d(kernel_size = 2, stride = 2)
        
        self.flatten = nn.Flatten()
        self.f5 = nn.Linear(in_features = 16*5*5, out_features = 120)
        self.f6 = nn.Linear(in_features = 120, out_features = 84)
        self.f7 = nn.Linear(in_features = 84, out_features = 10)
    
    def forward(self, x):  
        x = self.conv1(x)
        x = self.sig(x)
        x = self.s2(x)
        x = self.conv3(x)
        x = self.sig(x)
        x = self.s4(x)
        x = self.flatten(x)
        x = self.f5(x)
        x = self.sig(x)
        x = self.f6(x)
        x = self.sig(x)
        x = self.f7(x)
        
        return x
    
if __name__ == "__main__": # 在包中调试
    # device = torch.device("mps" if torch.backends.mps.is_built() else "cpu")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = LeNet().to(device) # 模型实例化
    print(summary(model, input_size = (1, 28, 28))) #通道数为1
    # 156 = (5*5*1)*6 + 6
    # 2416 = (5*5*6)*16 + 16
    # 48120 = 16*5*5*120 + 120
    # 10380 = 120*84 + 84
    # 850 = 84*10 + 10 
    # batch大小不影响参数量

